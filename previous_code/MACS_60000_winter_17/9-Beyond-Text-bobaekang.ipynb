{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 9 - Beyond Text\n",
    "\n",
    "This week, we \"trascend\" text to explore analysis of sound and visual content. Trillions of digital audio, image, and video files have been generated by cell phones and distributed sensors, preserved and shared through social medial, the web, private and government administrations. In this notebook, we read in and visualize audio and image files, process them to extract relevant features and measurement, then begin to explore how to analyze and extract information from them through the same approaches to supervised and unsupervised learning we have performed thoughout the quarter with text.\n",
    "\n",
    "For this notebook we will use the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#All these packages need to be installed from pip\n",
    "import scipy #For frequency analysis\n",
    "import scipy.fftpack\n",
    "import nltk #the Natural Language Toolkit\n",
    "import requests #For downloading our datasets\n",
    "import numpy as np #for arrays\n",
    "import pandas #gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import IPython #To show stuff\n",
    "\n",
    "#Image handling install as Pillow\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import PIL.ImageOps\n",
    "\n",
    "#install as scikit-image, this does the image manupulation\n",
    "import skimage\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import closing, square\n",
    "from skimage.color import label2rgb\n",
    "from skimage import data\n",
    "from skimage.feature import blob_dog, blob_log, blob_doh\n",
    "from skimage.future import graph\n",
    "from skimage import data, segmentation, color, filters, io\n",
    "from skimage.util.colormap import viridis\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "#these three do audio handling\n",
    "import pydub #Requires ffmpeg to be installed\n",
    "import speech_recognition #install as speechrecognition\n",
    "import soundfile #Install as pysoundfile \n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning it may generate.\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import csv\n",
    "import re\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Audio analysis \n",
    "\n",
    "First we will consider media that predates written language...sound and spoken language. Audio (and video) files come in two major categories, lossy or lossless. Lossless files save all information the microphone recorded. Lossy files, by contrast, drop sections humans are unlikely to notice. Recorded frequencies for both types are then typically compressed, which introduces further loss. To work with audio files, we want a format that is preferably lossless or minimally compressed. We will work with `wav` files here. Note that `mp3` is not acceptable. If you do not have `wav` files, we can use python to convert to `wav`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that read in 10 audio files (e.g., produced on your smartphone recorder?) from at least two different speakers, which include sentences of different types (e.g., question, statement, exclamation). At least two of these should include recordings of the two speakers talking to each other (e.g., a simple question/answer). Contrast the frequency distributions of the words spoken within speaker. What speaker's voice has a higher and which has lower frequency? What words are spoken at the highest and lowest frequencies? What parts-of-speech tend to be high or low? How do different types of sentences vary in their frequency differently? When people are speaking to each other, how do their frequencies change? Whose changes more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "samplePath = 'data/audio_samples/SBC060.mp3'\n",
    "transcriptPath = 'data/audio_samples/SBC060.trn'\n",
    "wavPath = '{}.wav'.format('.'.join(samplePath.split('.')[:-1]))\n",
    "IPython.display.Audio(samplePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sanders1Path = 'mydata/Sanders1.m4a'\n",
    "Sanders2Path = 'mydata/Sanders2.m4a'\n",
    "Sanders3Path = 'mydata/Sanders3.m4a'\n",
    "Sanders4Path = 'mydata/Sanders4.m4a'\n",
    "Trump1Path = 'mydata/Trump1.m4a'\n",
    "Trump2Path = 'mydata/Trump2.m4a'\n",
    "Trump3Path = 'mydata/Trump3.m4a'\n",
    "Trump4Path = 'mydata/Trump4.m4a'\n",
    "RapPath = 'mydata/Rap.m4a'\n",
    "Hey_EugenePath = 'mydata/Hey_Eugene.m4a'\n",
    "\n",
    "Sanders1trnPath = 'mydata/Sanders1.trn'\n",
    "Sanders2trnPath = 'mydata/Sanders2.trn'\n",
    "Sanders3trnPath = 'mydata/Sanders3.trn'\n",
    "Sanders4trnPath = 'mydata/Sanders4.trn'\n",
    "Trump1trnPath = 'mydata/Trump1.trn'\n",
    "Trump2trnPath = 'mydata/Trump2.trn'\n",
    "Trump3trnPath = 'mydata/Trump3.trn'\n",
    "Trump4trnPath = 'mydata/Trump4.trn'\n",
    "RaptrnPath = 'mydata/Rap.trn'\n",
    "Hey_EngenetrnPath = 'mydata/Hey_Eugene.trn'\n",
    "\n",
    "Sanders1wavPath = '{}.wav'.format('.'.join(Sanders1Path.split('.')[:-1]))\n",
    "Sanders2wavPath = '{}.wav'.format('.'.join(Sanders2Path.split('.')[:-1]))\n",
    "Sanders3wavPath = '{}.wav'.format('.'.join(Sanders3Path.split('.')[:-1]))\n",
    "Sanders4wavPath = '{}.wav'.format('.'.join(Sanders4Path.split('.')[:-1]))\n",
    "Trump1wavPath = '{}.wav'.format('.'.join(Trump1Path.split('.')[:-1]))\n",
    "Trump2wavPath = '{}.wav'.format('.'.join(Trump2Path.split('.')[:-1]))\n",
    "Trump3wavPath = '{}.wav'.format('.'.join(Trump3Path.split('.')[:-1]))\n",
    "Trump4wavPath = '{}.wav'.format('.'.join(Trump4Path.split('.')[:-1]))\n",
    "RapwavPath = '{}.wav'.format('.'.join(RapPath.split('.')[:-1]))\n",
    "Hey_EugenewavPath = '{}.wav'.format('.'.join(Hey_EugenePath.split('.')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "IPython.display.Audio(Trump1Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We are using a different package to convert than the in the rest of the code\n",
    "def convertToWAV(sourceFile, outputFile, overwrite = False):\n",
    "    if os.path.isfile(outputFile) and not overwrite:\n",
    "        print(\"{} exists already\".format(outputFile))\n",
    "        return\n",
    "    #Naive format extraction\n",
    "    sourceFormat = sourceFile.split('.')[-1]\n",
    "    sound = pydub.AudioSegment.from_file(sourceFile, format=sourceFormat)\n",
    "    sound.export(outputFile, format=\"wav\")\n",
    "    print(\"{} created\".format(outputFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert audio files from m4a to wav\n",
    "convertToWAV(Sanders1Path, Sanders1wavPath)\n",
    "convertToWAV(Sanders2Path, Sanders2wavPath)\n",
    "convertToWAV(Sanders3Path, Sanders3wavPath)\n",
    "convertToWAV(Sanders4Path, Sanders4wavPath)\n",
    "convertToWAV(Trump1Path, Trump1wavPath)\n",
    "convertToWAV(Trump2Path, Trump2wavPath)\n",
    "convertToWAV(Trump3Path, Trump3wavPath)\n",
    "convertToWAV(Trump4Path, Trump4wavPath)\n",
    "convertToWAV(RapPath, RapwavPath)\n",
    "convertToWAV(Hey_EugenePath, Hey_EugenewavPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we have created our `wav` file, notice that it is much large than the source `mp3`. We can load it with `soundfile` and work with it as a numpy data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "San1soundArr, San1soundRate = soundfile.read(Sanders1wavPath)\n",
    "San2soundArr, San2soundRate = soundfile.read(Sanders2wavPath)\n",
    "San3soundArr, San3soundRate = soundfile.read(Sanders3wavPath)\n",
    "San4soundArr, San4soundRate = soundfile.read(Sanders4wavPath)\n",
    "Tr1soundArr, Tr1soundRate = soundfile.read(Trump1wavPath)\n",
    "Tr2soundArr, Tr2soundRate = soundfile.read(Trump2wavPath)\n",
    "Tr3soundArr, Tr3soundRate = soundfile.read(Trump3wavPath)\n",
    "Tr4soundArr, Tr4soundRate = soundfile.read(Trump4wavPath)\n",
    "RapsoundArr, RapsoundRate = soundfile.read(RapwavPath)\n",
    "HEsoundArr, HEsoundRate = soundfile.read(Hey_EugenewavPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Tr1soundArr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is the raw data as a column array, which contains two channels (Left and Right) of the recording device. Some files, of course, will have more columns. The array comprises a series of numbers that measure the location of the speaker membrane (0=resting location). By quickly and rhythmically changing the location a note can be achieved. The larger the variation from the center, the louder the sound; the faster the oscillations, the higher the pitch. (The center of the oscillations does not have to be 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Tr1soundRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The other piece of information we get is the sample rate. This tells us how many measurements made per second, which allows us to know how long the entire recording is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "San1numS = San1soundArr.shape[0] // San1soundRate\n",
    "San2numS = San2soundArr.shape[0] // San2soundRate\n",
    "San3numS = San3soundArr.shape[0] // San3soundRate\n",
    "San4numS = San4soundArr.shape[0] // San4soundRate\n",
    "Tr1numS = Tr1soundArr.shape[0] // Tr1soundRate\n",
    "Tr2numS = Tr2soundArr.shape[0] // Tr2soundRate\n",
    "Tr3numS = Tr3soundArr.shape[0] // Tr3soundRate\n",
    "Tr4numS = Tr4soundArr.shape[0] // Tr4soundRate\n",
    "RapnumS = RapsoundArr.shape[0] // RapsoundRate\n",
    "HEnumS = HEsoundArr.shape[0] // HEsoundRate\n",
    "\n",
    "print(\"The Sanders1 is {} seconds long\".format(San1numS))\n",
    "print(\"Or {:.2f} minutes\".format(San1numS / 60.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's look at the first five seconds of the recordings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(San1soundArr[:San1soundRate*5]) # Sanders1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(San2soundArr[:San2soundRate*5]) # Sanders2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(San3soundArr[:San3soundRate*5]) # Sanders3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(San4soundArr[:San4soundRate*5]) # Sanders4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(Tr1soundArr[:Tr1soundRate*5]) # Trump1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(Tr2soundArr[:Tr2soundRate*5]) # Trump2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(Tr3soundArr[:Tr3soundRate*5]) # Trump3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(Tr4soundArr[:Tr4soundRate*5]) # Trump4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(RapsoundArr[:RapsoundRate*5]) # Rap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(HEsoundArr[:HEsoundRate*5]) # Hey Eugene!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that use the 10 audio files from at least two different speakers read in previously, attempt to automatically extract the words from Google, and calculate the word-error rate, as descibed in Chapter 9 from *Jurafsky & Martin*, page 334. How well does it do? Under what circumstances does it perform poorly? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Speech-to-Text\n",
    "\n",
    "We can also do speech recognition on audio, but this requires a complex machine learning system. Luckily there are many online services to do this. We have a function that uses Google's API. There are two API's: one is free but limited; the other is commercial and you can provide the function `speechRec` with a file containing the API keys, using `jsonFile=` if you wish. For more about this look [here](https://stackoverflow.com/questions/38703853/how-to-use-google-speech-recognition-api-in-python) or the `speech_recognition` [docs](https://github.com/Uberi/speech_recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Using another library so we need to use files again\n",
    "def speechRec(targetFile, language = \"en-US\", raw = False, jsonFile = 'data/googleAPIKeys.json'):\n",
    "    r = speech_recognition.Recognizer()\n",
    "    if not os.path.isfile(jsonFile):\n",
    "        jsonString = None\n",
    "    else:\n",
    "        with open(jsonFile) as f:\n",
    "            jsonString = f.read()\n",
    "    with speech_recognition.AudioFile(targetFile) as source:\n",
    "        audio = r.record(source)\n",
    "    try:\n",
    "        if jsonString is None:\n",
    "            print(\"Sending data to Google Speech Recognition\")\n",
    "            dat =  r.recognize_google(audio)\n",
    "        else:\n",
    "            print(\"Sending data to Google Cloud Speech\")\n",
    "            dat =  r.recognize_google_cloud(audio, credentials_json=jsonString)\n",
    "    except speech_recognition.UnknownValueError:\n",
    "        print(\"Google could not understand audio\")\n",
    "    except speech_recognition.RequestError as e:\n",
    "        print(\"Could not request results from Google service; {0}\".format(e))\n",
    "    else:\n",
    "        print(\"Success\")\n",
    "        return dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The example above is of too low quality so we will be using another file `data/audio_samples/english.wav`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "speechRec('mydata/Sanders1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "speechRec('mydata/Sanders2.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "speechRec('mydata/Sanders3.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "speechRec('mydata/Sanders4.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "speechRec('mydata/Trump1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "speechRec('mydata/Trump2.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "speechRec('mydata/Trump3.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "speechRec('mydata/Trump4.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "speechRec('mydata/Rap.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "speechRec('mydata/Hey_Eugene.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that read in 10 image files (e.g., produced on your smartphone, harvested from the web, etc.) that feature different kinds of objects and settings, including at least one indoor and one outdoor setting. Perform blob detection and RAG segmentation using the approaches modeled above. How well does each algorithm identify objects or features of interest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image analysis\n",
    "\n",
    "Now we will explore image files. First, we will read in an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image1 = PIL.Image.open('mydata/image1.jpg')\n",
    "image2 = PIL.Image.open('mydata/image2.jpg')\n",
    "image3 = PIL.Image.open('mydata/image3.jpg')\n",
    "image4 = PIL.Image.open('mydata/image4.jpg')\n",
    "image5 = PIL.Image.open('mydata/image5.jpg')\n",
    "image6 = PIL.Image.open('mydata/image6.jpg')\n",
    "image7 = PIL.Image.open('mydata/image7.jpg')\n",
    "image8 = PIL.Image.open('mydata/image8.jpg')\n",
    "image9 = PIL.Image.open('mydata/image9.jpg')\n",
    "image10 = PIL.Image.open('mydata/image10.jpg')\n",
    "\n",
    "imageArr1 = np.asarray(image1)\n",
    "imageArr2 = np.asarray(image2)\n",
    "imageArr3 = np.asarray(image3)\n",
    "imageArr4 = np.asarray(image4)\n",
    "imageArr5 = np.asarray(image5)\n",
    "imageArr6 = np.asarray(image6)\n",
    "imageArr1 = np.asarray(image7)\n",
    "imageArr2 = np.asarray(image8)\n",
    "imageArr3 = np.asarray(image9)\n",
    "imageArr4 = np.asarray(image10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The image we have loaded is a raster image, meaning it is a grid of pixels, each pixel contains 1-4 numbers giving the amounts of color contained in it. In this case, we can see it has 3 values per pixel, these are RGB or Red, Green and Blue. If we want to see just the green we can look at just that array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(imageArr1[:,:,2], cmap='Greens') #The order is R G B, so 2 is the Green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(imageArr1[:,:,1], cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(imageArr1[:,:,0], cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(imageArr2[:,:,2], cmap='Greens') #The order is R G B, so 2 is the Green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(imageArr2[:,:,1], cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(imageArr2[:,:,0], cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image_gray1 = PIL.ImageOps.invert(image1.convert('L'))\n",
    "image_gray2 = PIL.ImageOps.invert(image2.convert('L'))\n",
    "image_gray3 = PIL.ImageOps.invert(image3.convert('L'))\n",
    "image_gray4 = PIL.ImageOps.invert(image4.convert('L'))\n",
    "image_gray5 = PIL.ImageOps.invert(image5.convert('L'))\n",
    "image_gray6 = PIL.ImageOps.invert(image6.convert('L'))\n",
    "image_gray7 = PIL.ImageOps.invert(image7.convert('L'))\n",
    "image_gray8 = PIL.ImageOps.invert(image8.convert('L'))\n",
    "image_gray9 = PIL.ImageOps.invert(image9.convert('L'))\n",
    "image_gray10 = PIL.ImageOps.invert(image10.convert('L'))\n",
    "\n",
    "image_grayArr1 = np.asarray(image_gray1)\n",
    "image_grayArr2 = np.asarray(image_gray2)\n",
    "image_grayArr3 = np.asarray(image_gray3)\n",
    "image_grayArr4 = np.asarray(image_gray4)\n",
    "image_grayArr5 = np.asarray(image_gray5)\n",
    "image_grayArr6 = np.asarray(image_gray6)\n",
    "image_grayArr7 = np.asarray(image_gray7)\n",
    "image_grayArr8 = np.asarray(image_gray8)\n",
    "image_grayArr9 = np.asarray(image_gray9)\n",
    "image_grayArr10 = np.asarray(image_gray10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A grayscale image is defined by its pixel intensities (and a color image can be defined by its red, green, blue pixel intensities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(image_gray1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(image_gray2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Blob Detection\n",
    "\n",
    "Recall our earlier use of scikit-learn for machine learning. Now we will use scikit-image to do some simple image processing. Here we will perform three operations for 'blob' of simple object detection. In computer vision, blob detection methods aim to detect regions in a digital image that differ in properties, such as brightness or color, compared to surrounding regions. Informally, a blob is a region of an image in which some properties are approximately constant or similar to each other. We will do this in three ways.\n",
    "\n",
    "First, we will take the Laplacian of an image, which is a 2-D isotropic (applying equally well in all directions) measure of the 2nd spatial derivative of an image. The Laplacian of an image highlights regions of rapid intensity change and is therefore often used for edge detection. This Laplacian is taken of the image once a Gaussian smoothing filter has been applied in order to reduce its sensitivity to noise.\n",
    "\n",
    "The Laplacian $L(x,y)$ of an image with pixel intensity values $I(x,y)$ is given by: $L(x,y)=\\frac{\\delta^2x}{\\delta x^2} + \\frac{\\delta^2y}{\\delta y^2}$. A Gaussian smoothing filter takes a 2 dimensional Guassian, $G(x,y)=\\frac{1}{2 \\pi \\sigma^2} e^\\frac{-x^2 + y^2}{2\\sigma^2}$, which looks like: <img src=\"http://www.librow.com/content/common/images/articles/article-9/2d_distribution.gif\">\n",
    "\n",
    "This Gaussian *kernel* is applied to the pixel intensities of the image via *convolution* -- the kernel is multiplied by the pixel intensities, while centered on each pixel, then added.\n",
    "\n",
    "The blob detector computes the Laplacian of Gaussian (LoG) images with successively increasing standard deviation and stacks them up in a cube. Blobs are local maximas within this cube. Detecting larger blobs is slower because of larger kernel sizes during convolution. Bright blobs on dark backgrounds are detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "blobs_log1 = blob_log(image_grayArr1, max_sigma=30, num_sigma=10, threshold=.1)\n",
    "blobs_log1[:, 2] = blobs_log1[:, 2] * sqrt(2) #Radi\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.imshow(image_gray1, interpolation='nearest')\n",
    "for blob in blobs_log1:\n",
    "    y, x, r = blob\n",
    "    c = plt.Circle((x, y), r, linewidth=2, fill=False)\n",
    "    ax.add_patch(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "blobs_log2 = blob_log(image_grayArr2, max_sigma=30, num_sigma=10, threshold=.1)\n",
    "blobs_log2[:, 2] = blobs_log2[:, 2] * sqrt(2) #Radi\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.imshow(image_gray2, interpolation='nearest')\n",
    "for blob in blobs_log2:\n",
    "    y, x, r = blob\n",
    "    c = plt.Circle((x, y), r, linewidth=2, fill=False)\n",
    "    ax.add_patch(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "blobs_log3 = blob_log(image_grayArr3, max_sigma=30, num_sigma=10, threshold=.1)\n",
    "blobs_log3[:, 2] = blobs_log3[:, 2] * sqrt(2) #Radi\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.imshow(image_gray, interpolation='nearest')\n",
    "for blob in blobs_log:\n",
    "    y, x, r = blob\n",
    "    c = plt.Circle((x, y), r, linewidth=2, fill=False)\n",
    "    ax.add_patch(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Second, we look at Difference of Gaussian (DoG), a much faster approximation of the LoG approach in which an image is blurred with increasing standard deviations and the difference between two successively blurred images are stacked up in a cube. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "blobs_dog1 = blob_dog(image_grayArr1, max_sigma=30, threshold=.1)\n",
    "blobs_dog1[:, 2] = blobs_dog1[:, 2] * sqrt(2)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.imshow(image_gray1, interpolation='nearest')\n",
    "for blob in blobs_dog1:\n",
    "    y, x, r = blob\n",
    "    c = plt.Circle((x, y), r, linewidth=2, fill=False)\n",
    "    ax.add_patch(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "blobs_dog2 = blob_dog(image_grayArr2, max_sigma=30, threshold=.1)\n",
    "blobs_dog2[:, 2] = blobs_dog2[:, 2] * sqrt(2)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.imshow(image_gray2, interpolation='nearest')\n",
    "for blob in blobs_dog2:\n",
    "    y, x, r = blob\n",
    "    c = plt.Circle((x, y), r, linewidth=2, fill=False)\n",
    "    ax.add_patch(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "blobs_dog3 = blob_dog(image_grayArr3, max_sigma=30, threshold=.1)\n",
    "blobs_dog3[:, 2] = blobs_dog3[:, 2] * sqrt(2)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.imshow(image_gray3, interpolation='nearest')\n",
    "for blob in blobs_dog3:\n",
    "    y, x, r = blob\n",
    "    c = plt.Circle((x, y), r, linewidth=2, fill=False)\n",
    "    ax.add_patch(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, we consider the Determinant of Hessian (DoH) approach. The Hessian matrix or Hessian is a square matrix of second-order partial derivatives $\\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(x_1^{*}, \\ldots, x_n^{*})$ and is calculated on square pixel patches of the image. The determinant is the scaling factor of each patch. This approach is fastest and detects blobs by finding maximas in this matrix (of the Determinant of the Hessian of the image). Detection speed is independent of the size of blobs as the implementation uses box filters, $\\begin{bmatrix}1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1\\end{bmatrix}$, instead of Gaussians for the convolution. As a result, small blobs (< 3 pixels) cannot be detected accurately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "blobs_doh1 = blob_doh(image_gray1, max_sigma=30, threshold=.01)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.imshow(image_gray1, interpolation='nearest')\n",
    "for blob in blobs_doh1:\n",
    "    y, x, r = blob\n",
    "    c = plt.Circle((x, y), r, linewidth=2, fill=False)\n",
    "    ax.add_patch(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "blobs_doh2 = blob_doh(image_gray2, max_sigma=30, threshold=.01)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.imshow(image_gray2, interpolation='nearest')\n",
    "for blob in blobs_doh2:\n",
    "    y, x, r = blob\n",
    "    c = plt.Circle((x, y), r, linewidth=2, fill=False)\n",
    "    ax.add_patch(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "blobs_doh3 = blob_doh(image_gray3, max_sigma=30, threshold=.01)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.imshow(image_gray3, interpolation='nearest')\n",
    "for blob in blobs_doh3:\n",
    "    y, x, r = blob\n",
    "    c = plt.Circle((x, y), r, linewidth=2, fill=False)\n",
    "    ax.add_patch(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Humans possess an incredible ability to identify objects in an image. Segmentation is the process of dividing an image into meaningful regions. All pixels belonging to a region should receive a unique label in an ideal segmentation.\n",
    "\n",
    "Region Adjacency Graphs (RAGs) are a common data structure for many segmentation algorithms. First, we define regions through the SLIC algorithm that assigns a unique label to each region or a localized cluster of pixels sharing some similar property (e.g., color or grayscale intensity). Then we'll consider each region a node in a graph, and construct a region boundary RAG, where the edge weight between two regions is the average value of the corresponding pixels in edge_map along their shared boundary. Then edges below a specified threshold are removed and a connected component is labeled as one region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels1 = segmentation.slic(image_gray1, compactness=30, n_segments=100)\n",
    "edges1 = filters.sobel(image_grayArr1)\n",
    "edges_rgb1 = color.gray2rgb(edges1)\n",
    "g1 = graph.rag_boundary(labels1, edges1)\n",
    "out1 = graph.draw_rag(labels1, g1, edges_rgb1, node_color=\"#999999\", colormap=viridis)\n",
    "\n",
    "labels2 = segmentation.slic(image_gray2, compactness=30, n_segments=100)\n",
    "edges2 = filters.sobel(image_grayArr2)\n",
    "edges_rgb2 = color.gray2rgb(edges2)\n",
    "g2 = graph.rag_boundary(labels2, edges2)\n",
    "out2 = graph.draw_rag(labels2, g2, edges_rgb2, node_color=\"#999999\", colormap=viridis)\n",
    "\n",
    "labels3 = segmentation.slic(image_gray3, compactness=30, n_segments=100)\n",
    "edges3 = filters.sobel(image_grayArr3)\n",
    "edges_rgb3 = color.gray2rgb(edges3)\n",
    "g3 = graph.rag_boundary(labels3, edges3)\n",
    "out3 = graph.draw_rag(labels3, g3, edges_rgb3, node_color=\"#999999\", colormap=viridis)\n",
    "\n",
    "labels4 = segmentation.slic(image_gray4, compactness=30, n_segments=100)\n",
    "edges4 = filters.sobel(image_grayArr4)\n",
    "edges_rgb4 = color.gray2rgb(edges4)\n",
    "g4 = graph.rag_boundary(labels4, edges4)\n",
    "out4 = graph.draw_rag(labels4, g4, edges_rgb4, node_color=\"#999999\", colormap=viridis)\n",
    "\n",
    "labels5 = segmentation.slic(image_gray5, compactness=30, n_segments=100)\n",
    "edges5 = filters.sobel(image_grayArr5)\n",
    "edges_rgb5 = color.gray2rgb(edges5)\n",
    "g5 = graph.rag_boundary(labels5, edges5)\n",
    "out5 = graph.draw_rag(labels5, g5, edges_rgb5, node_color=\"#999999\", colormap=viridis)\n",
    "\n",
    "labels6 = segmentation.slic(image_gray6, compactness=30, n_segments=100)\n",
    "edges6 = filters.sobel(image_grayArr6)\n",
    "edges_rgb6 = color.gray2rgb(edges6)\n",
    "g6 = graph.rag_boundary(labels6, edges6)\n",
    "out6 = graph.draw_rag(labels6, g6, edges_rgb6, node_color=\"#999999\", colormap=viridis)\n",
    "\n",
    "labels7 = segmentation.slic(image_gray7, compactness=30, n_segments=100)\n",
    "edges7 = filters.sobel(image_grayArr7)\n",
    "edges_rgb7 = color.gray2rgb(edges7)\n",
    "g7 = graph.rag_boundary(labels7, edges7)\n",
    "out7 = graph.draw_rag(labels7, g7, edges_rgb7, node_color=\"#999999\", colormap=viridis)\n",
    "\n",
    "labels8 = segmentation.slic(image_gray8, compactness=30, n_segments=100)\n",
    "edges8 = filters.sobel(image_grayArr8)\n",
    "edges_rgb8 = color.gray2rgb(edges8)\n",
    "g8 = graph.rag_boundary(labels8, edges8)\n",
    "out8 = graph.draw_rag(labels8, g8, edges_rgb8, node_color=\"#999999\", colormap=viridis)\n",
    "\n",
    "labels9 = segmentation.slic(image_gray9, compactness=30, n_segments=100)\n",
    "edges9 = filters.sobel(image_grayArr9)\n",
    "edges_rgb9 = color.gray2rgb(edges9)\n",
    "g9 = graph.rag_boundary(labels9, edges9)\n",
    "out9 = graph.draw_rag(labels9, g9, edges_rgb9, node_color=\"#999999\", colormap=viridis)\n",
    "\n",
    "labels10 = segmentation.slic(image_gray10, compactness=30, n_segments=100)\n",
    "edges10 = filters.sobel(image_grayArr10)\n",
    "edges_rgb10 = color.gray2rgb(edges10)\n",
    "g10 = graph.rag_boundary(labels10, edges10)\n",
    "out10 = graph.draw_rag(labels10, g10, edges_rgb10, node_color=\"#999999\", colormap=viridis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "io.imshow(out1)io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "io.imshow(out2)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "io.imshow(out3)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "io.imshow(out4)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "io.imshow(out5)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "io.imshow(out6)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "io.imshow(out7)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "io.imshow(out8)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "io.imshow(out9)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "io.imshow(out10)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "editable": true
   },
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that report the results from experiments in which you place each of images taken or retrieved for the last exercise through the online demos for [caffe](http://demo.caffe.berkeleyvision.org) and [places](http://places.csail.mit.edu/demo.html). Paste the image and the output for both object detector and scene classifier below, beside one another. Calculate precision and recall for caffe's ability to detect objects of interest across your images. What do you think about Places' scene categories and their assignments to your images? What would be improved labels for your images? Could you use image classification to enhance your research project and, if so, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Object Detection & Scene Classification\n",
    "\n",
    "Modern image and video analysis is typically performed using deep learning implemented as layers of convolutional neural nets to classify scenes and to detect and label objects. To learn more about deep learning and convolutional neural networks, spend some time with Andrew Ng's excellent [tutorial](http://ufldl.stanford.edu/tutorial/). Because such algorithms require substantial computing power, none of the high-quality classifiers or detectors currently available are implemented in python, although many can be called via api. The most popular open source image object detector is UC Berkeley's [*caffe*](http://caffe.berkeleyvision.org) library of trained and trainable neural nets written in C++. (Check out the [python api](https://github.com/BVLC/caffe/blob/master/python/caffe/pycaffe.py)). Scene classifiers can be built on top of caffe, such as MIT's [Places](http://places.csail.mit.edu). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caffe classifications\n",
    "The following images are screenshopts of Caffe classifications of my 6 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/caffe1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/caffe2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/caffe3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/caffe4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/caffe5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/caffe6.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/caffe7.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/caffe8.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/caffe9.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/caffe10.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/caffe11.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/caffe12.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place\n",
    "The following images are screenshots of Place classifications of my six images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/place1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/place2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/place3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/place4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/place5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image('mydata/place6.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "I have imported above screenshots of caffe and place classification results of the same six images I used previously. For caffe, I would say that the classification by algorithm is accurate for 50% of the images: more specifically, the second, third and fourth images. The algorithm classifies the first image as 'bearskin', about which I have no clue. \n",
    "\n",
    "Although there is not a direct way to incorporate the image classification technology into my current project, It may be interesting to compare the cover pages or photographs of the same subject used by different newspapers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
